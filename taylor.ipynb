{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 10000\n",
    "sample_dim = 10\n",
    "noise_scale = 1.0\n",
    "train_split = 0.8\n",
    "theta_range = 1.0\n",
    "data_range = 500\n",
    "epoch = 50000\n",
    "lr = 1e-5\n",
    "dim_per_task = 7\n",
    "task_num = 10\n",
    "shift_scale = 1e-1\n",
    "tasks = [np.random.randint(0, sample_dim, dim_per_task) for _ in range(task_num)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGen:\n",
    "    def __init__(self, sample_size, sample_dim, noise_scale, train_split, theta_range, data_range, tasks):\n",
    "        self.sample_size = sample_size\n",
    "        self.sample_dim = sample_dim\n",
    "        self.noise_scale = noise_scale\n",
    "        self.train_split = train_split\n",
    "        self.theta_range = theta_range\n",
    "        self.data_range = data_range\n",
    "        self.theta = np.random.uniform(-theta_range, theta_range, (sample_dim, 1))\n",
    "        self.tasks = tasks\n",
    "    \n",
    "    def get_data(self):\n",
    "        for task in self.tasks:\n",
    "            self.theta += shift_scale*self.noise_scale*np.random.normal(0, 1, (self.sample_dim, 1))\n",
    "            X = np.zeros((self.sample_size, self.sample_dim))\n",
    "            X[:, task] = np.random.uniform(-self.data_range, self.data_range, (self.sample_size, self.sample_dim))[:, task]\n",
    "            y = np.dot(X, self.theta).squeeze() + self.noise_scale * np.random.normal(0, 1, self.sample_size)\n",
    "            train_size = int(self.sample_size * self.train_split)\n",
    "            X_train, X_test = torch.tensor(X[:train_size], dtype=torch.float), torch.tensor(X[train_size:], dtype=torch.float)\n",
    "            y_train, y_test = torch.tensor(y[:train_size], dtype=torch.float), torch.tensor(y[train_size:], dtype=torch.float)\n",
    "            yield X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, sample_dim, lr):\n",
    "        super().__init__()\n",
    "        self.theta = torch.nn.Parameter(torch.randn(sample_dim, 1))\n",
    "        self.optimizer = torch.optim.SGD(self.parameters(), lr=lr)\n",
    "        self.loss_fn = torch.nn.MSELoss()\n",
    "    \n",
    "    def set_data(self, X, y):\n",
    "        self.X, self.y = X.detach().clone(), y.detach().clone()\n",
    "\n",
    "    def forward(self, X):\n",
    "        return torch.matmul(X, self.theta).squeeze()\n",
    "    \n",
    "    def train(self, X, y, epoch, lr):\n",
    "        for _ in range(epoch):\n",
    "            self.optimizer.zero_grad()\n",
    "            loss = self.loss_fn(self(X), y)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "    \n",
    "    def test(self, X, y):\n",
    "        return self.loss_fn(self(X), y)\n",
    "    \n",
    "    def get_loss_with_theta(self, theta):\n",
    "        return self.loss_fn(torch.matmul(self.X, theta).squeeze(), self.y)\n",
    "    \n",
    "    def get_gradient(self, X, y, theta):\n",
    "        self.set_data(X, y)\n",
    "        return autograd.functional.jacobian(self.get_loss_with_theta, theta.squeeze()).detach().clone()\n",
    "    \n",
    "    def get_hessian(self, X, y, theta):\n",
    "        self.set_data(X, y)\n",
    "        return autograd.functional.hessian(self.get_loss_with_theta, theta.squeeze()).detach().clone()\n",
    "    \n",
    "    def prepare_estimation(self, X, y, old_theta):\n",
    "        self.gradient = self.get_gradient(X, y, old_theta).reshape(1, -1)\n",
    "        self.hessian = self.get_hessian(X, y, old_theta)\n",
    "        self.old_theta = old_theta.detach().clone()\n",
    "        self.old_ans = self.get_loss_with_theta(old_theta).detach().clone()\n",
    "    \n",
    "    def estimate(self, theta):\n",
    "        dtheta = theta - self.old_theta\n",
    "        ans = self.old_ans.detach().clone()\n",
    "        ans += torch.matmul(self.gradient, dtheta).squeeze()\n",
    "        ans += 0.5 * torch.matmul(torch.matmul(dtheta.T, self.hessian), dtheta).squeeze()\n",
    "        return ans\n",
    "    \n",
    "    def diag_estimate(self, theta):\n",
    "        dtheta = theta - self.old_theta\n",
    "        ans = self.old_ans.detach().clone()\n",
    "        ans += torch.matmul(self.gradient, dtheta).squeeze()\n",
    "        ans += 0.5 * torch.matmul(torch.matmul(dtheta.T, torch.diag(torch.diag(self.hessian))), dtheta).squeeze()\n",
    "        return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taylor Expansion Error Ratio: 6.95e-07, Diagonal Approximation Error Ratio: 4.96e-03\n",
      "Dominance Ratio: 7136.666666666667\n",
      "Taylor Expansion Error Ratio: 8.31e-07, Diagonal Approximation Error Ratio: 1.51e-02\n",
      "Dominance Ratio: 18113.5\n",
      "Taylor Expansion Error Ratio: 6.06e-07, Diagonal Approximation Error Ratio: 4.74e-03\n",
      "Dominance Ratio: 7827.5\n",
      "Taylor Expansion Error Ratio: 0.00e+00, Diagonal Approximation Error Ratio: 0.00e+00\n",
      "Dominance Ratio: INF\n",
      "Taylor Expansion Error Ratio: 6.10e-08, Diagonal Approximation Error Ratio: 0.00e+00\n",
      "Dominance Ratio: 0.0\n",
      "Taylor Expansion Error Ratio: 5.93e-07, Diagonal Approximation Error Ratio: 8.33e-03\n",
      "Dominance Ratio: 14038.444444444445\n",
      "Taylor Expansion Error Ratio: 3.74e-07, Diagonal Approximation Error Ratio: 7.24e-03\n",
      "Dominance Ratio: 19376.25\n",
      "Taylor Expansion Error Ratio: 1.33e-07, Diagonal Approximation Error Ratio: 7.86e-03\n",
      "Dominance Ratio: 59095.0\n",
      "Taylor Expansion Error Ratio: 5.75e-07, Diagonal Approximation Error Ratio: 1.84e-02\n",
      "Dominance Ratio: 31903.333333333332\n"
     ]
    }
   ],
   "source": [
    "model = Model(sample_dim, lr)\n",
    "Data = DataGen(sample_size, sample_dim, noise_scale, train_split, theta_range, data_range, tasks)\n",
    "first_task = True\n",
    "for X_train, y_train, X_test, y_test in Data.get_data():\n",
    "    model.train(X_train, y_train, epoch, lr)\n",
    "    if first_task:\n",
    "        first_task = False\n",
    "    else:\n",
    "        estimation = float(model.estimate(model.theta))\n",
    "        diag_estimation = float(model.diag_estimate(model.theta))\n",
    "        actual = float(model.test(model.X, model.y))\n",
    "        delta = float(np.abs(estimation - actual))\n",
    "        diag_delta = float(np.abs(diag_estimation - estimation))\n",
    "        print(f\"Taylor Expansion Error Ratio: {delta/actual:.2e}, Diagonal Approximation Error Ratio: {diag_delta/actual:.2e}\")\n",
    "        print(f\"Dominance Ratio: {'INF' if delta == 0 else diag_delta/delta}\")\n",
    "    model.prepare_estimation(X_train, y_train, model.theta)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('ewc')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "75bcbc7ecb8809637fbec3e0f1b4e88beb1d0779e970f7f640759102f9000d4d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
